{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "551027d1b939ec0016f2e01e6bd6ac0e4699d2ec5e4974e783e9fca478101a51"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Project 1 - Classification, weight sharing, auxiliary losses\n",
    "\n",
    "This notebook contains my ideas concerning the first question of the project. \n",
    "\n",
    ">The goal of the project is to compare different architectures, and assess the performance improvement\n",
    "that can be achieved through weight sharing, or using auxiliary losses. For the latter, the training can\n",
    "in particular take advantage of the availability of the classes of the two digits in each pair, beside the\n",
    "Boolean value truly of interest. \n",
    "\n",
    ">All the experiments should be done with 1000 pairs for training and test. A convnet with around 70000\n",
    "parameters can be trained with 25 epochs in the VM in less than 2s and should achieve around 15% error\n",
    "rate. \n",
    "\n",
    ">Performance estimates provided in your report should be estimated through 10+ rounds for each\n",
    "architecture, where both data and weight initialization are randomized, and you should provide estimates\n",
    "of standard deviations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torchvision import datasets\n",
    "\n",
    "# Import function\n",
    "from dlc_practical_prologue import generate_pair_sets\n",
    "\n",
    "# Matplot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "## Import data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1000, 2, 14, 14])\ntorch.Size([1000])\ntorch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = generate_pair_sets(nb=1000)\n",
    "\n",
    "# Print dimensions\n",
    "print(train_input.size())\n",
    "print(train_target.size())\n",
    "print(train_classes.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "train_mean = train_input.mean()\n",
    "train_std = train_input.std()\n",
    "\n",
    "train_input -= train_mean\n",
    "test_input -= train_mean\n",
    "\n",
    "train_input /= train_std\n",
    "test_input /= train_std"
   ]
  },
  {
   "source": [
    "## Defining the network\n",
    "\n",
    "- How to use multiple losses in PyTorch: https://stackoverflow.com/questions/53994625/how-can-i-process-multi-loss-in-pytorch/53995165"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FelixNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16,\n",
    "                              kernel_size = 5,\n",
    "                              padding = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32,\n",
    "                               kernel_size = 3,\n",
    "                               padding = 2)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=16)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=32)\n",
    "        self.fc_numbers = nn.Linear(in_features=32, out_features=10)\n",
    "        self.fc_comparison = nn.Linear(in_features=64, out_features=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split input\n",
    "        x1 = x[:,0:1,:,:]\n",
    "        x2 = x[:,1:2,:,:]\n",
    "\n",
    "        # Define the shared part of the network\n",
    "        x1, y1_number = self.shared_forward(x1)\n",
    "        x2, y2_number = self.shared_forward(x2)\n",
    "\n",
    "        # Stacking outputs\n",
    "        out_aux = torch.cat((y1_number, y2_number), dim=0)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "        # Comparing numbers\n",
    "        y = self.fc_comparison(x)\n",
    "\n",
    "        return y, out_aux\n",
    "\n",
    "    def shared_forward(self, x):\n",
    "        # First layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Second layer\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Third layer\n",
    "        y = F.avg_pool2d(x, 16).view(x.size(0), -1)\n",
    "        y_numbers = self.fc_numbers(y)\n",
    "        return y, y_numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### Epoch   1: Target error =46.00%  Auxillary error=87.85% ###\n",
      "### Epoch  11: Target error =37.80%  Auxillary error=73.15% ###\n",
      "### Epoch  21: Target error =30.70%  Auxillary error=66.70% ###\n",
      "### Epoch  31: Target error =25.90%  Auxillary error=59.65% ###\n",
      "### Epoch  41: Target error =24.00%  Auxillary error=55.00% ###\n",
      "### Epoch  51: Target error =23.10%  Auxillary error=51.50% ###\n",
      "### Epoch  61: Target error =22.40%  Auxillary error=48.75% ###\n",
      "### Epoch  71: Target error =22.20%  Auxillary error=47.25% ###\n",
      "### Epoch  81: Target error =22.20%  Auxillary error=46.35% ###\n",
      "### Epoch  91: Target error =22.30%  Auxillary error=46.00% ###\n",
      "### Epoch 100: Target error =22.30%  Auxillary error=45.55% ###\n"
     ]
    }
   ],
   "source": [
    "### TRAINING ###\n",
    "model = FelixNet()\n",
    "\n",
    "# Parmas\n",
    "lr = 0.1\n",
    "mini_batch_size = 100\n",
    "nb_epochs = 100\n",
    "verbose = True\n",
    "print_every = int(0.1 * nb_epochs)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Lerning rate sheduler\n",
    "nb_steps = 10\n",
    "step_size = 1 if nb_epochs <= nb_steps else int(nb_epochs/nb_steps)\n",
    "gamma = 0.65\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Iterate over epochs\n",
    "for e in range(nb_epochs):\n",
    "    # Learning rate sheduler step\n",
    "    if e != 0: lr_scheduler.step()\n",
    "\n",
    "    # Set model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over minibatches\n",
    "    for b in range(0, train_input.size(0), mini_batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out, out_aux = model(train_input.narrow(0, b, mini_batch_size))\n",
    "\n",
    "        loss_target = criterion(out, train_target.narrow(0, b, mini_batch_size))\n",
    "        loss_aux = criterion(out_aux, train_classes.narrow(0, b, mini_batch_size).T.reshape(-1))\n",
    "        loss = loss_target + loss_aux\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if verbose and (e % print_every == 0 or e+1==nb_epochs):\n",
    "        # Put model in test model\n",
    "        model.eval()\n",
    "\n",
    "        err, err_aux = 0, 0\n",
    "\n",
    "        # Loop over testset and get outputs\n",
    "        for b in range(0, test_input.size(0), mini_batch_size):\n",
    "            out, out_aux = model(test_input.narrow(0, b, mini_batch_size))\n",
    "\n",
    "            out = torch.argmax(out, dim=1)\n",
    "            out_aux = torch.argmax(out_aux, dim=1)\n",
    "\n",
    "            err += (out != test_target.narrow(0, b, mini_batch_size)).sum()\n",
    "            err_aux += (out_aux != test_classes.narrow(0, b, mini_batch_size).T.reshape(-1)).sum()\n",
    "\n",
    "        print(\"### Epoch {:3d}: Target error ={:.2f}%  Auxillary error={:.2f}% ###\".format(e+1, err/test_input.size(0)*100, err_aux/test_input.size(0)*50))\n"
   ]
  }
 ]
}