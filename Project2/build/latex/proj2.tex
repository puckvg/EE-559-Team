%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{Proj2}
\date{May 19, 2021}
\release{1.0}
\author{PuckVG}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Project2}
\label{\detokenize{modules:project2}}\label{\detokenize{modules::doc}}

\section{nn package}
\label{\detokenize{nn:nn-package}}\label{\detokenize{nn::doc}}

\subsection{Submodules}
\label{\detokenize{nn:submodules}}

\subsection{nn.activation module}
\label{\detokenize{nn:module-nn.activation}}\label{\detokenize{nn:nn-activation-module}}\index{module@\spxentry{module}!nn.activation@\spxentry{nn.activation}}\index{nn.activation@\spxentry{nn.activation}!module@\spxentry{module}}\index{Activation (class in nn.activation)@\spxentry{Activation}\spxextra{class in nn.activation}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.activation.Activation}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{nn.activation.}}\sphinxbfcode{\sphinxupquote{Activation}}}
Bases: {\hyperref[\detokenize{nn:nn.module.Module}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{nn.module.Module}}}}}
\index{backward() (nn.activation.Activation method)@\spxentry{backward()}\spxextra{nn.activation.Activation method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.activation.Activation.backward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{backward}}}{\emph{\DUrole{n}{dy}}}{}
Compute gradients of input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{dy}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Backpropagated gradient from the next layer.

\item[{Returns}] \leavevmode
Gradient

\item[{Return type}] \leavevmode
grad (torch.tensor)

\end{description}\end{quote}

\end{fulllineitems}

\index{forward() (nn.activation.Activation method)@\spxentry{forward()}\spxextra{nn.activation.Activation method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.activation.Activation.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}}{}
Compute the activation.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Input tensor.

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{ReLU (class in nn.activation)@\spxentry{ReLU}\spxextra{class in nn.activation}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.activation.ReLU}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{nn.activation.}}\sphinxbfcode{\sphinxupquote{ReLU}}}
Bases: {\hyperref[\detokenize{nn:nn.activation.Activation}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{nn.activation.Activation}}}}}

ReLU activation function
\index{forward() (nn.activation.ReLU method)@\spxentry{forward()}\spxextra{nn.activation.ReLU method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.activation.ReLU.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}}{}
Compute the activation.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Input tensor.

\item[{Returns}] \leavevmode
Output tensor.

\item[{Return type}] \leavevmode
out (torch.tensor)

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{Tanh (class in nn.activation)@\spxentry{Tanh}\spxextra{class in nn.activation}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.activation.Tanh}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{nn.activation.}}\sphinxbfcode{\sphinxupquote{Tanh}}}
Bases: {\hyperref[\detokenize{nn:nn.activation.Activation}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{nn.activation.Activation}}}}}

Tanh activation function
\index{forward() (nn.activation.Tanh method)@\spxentry{forward()}\spxextra{nn.activation.Tanh method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.activation.Tanh.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}}{}
Compute the activation.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Input tensor.

\item[{Returns}] \leavevmode
Output tensor.

\item[{Return type}] \leavevmode
out (torch.tensor)

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsection{nn.functional module}
\label{\detokenize{nn:module-nn.functional}}\label{\detokenize{nn:nn-functional-module}}\index{module@\spxentry{module}!nn.functional@\spxentry{nn.functional}}\index{nn.functional@\spxentry{nn.functional}!module@\spxentry{module}}
The functional.py contains the concrete implementations of specific functionals.
\index{d\_mse() (in module nn.functional)@\spxentry{d\_mse()}\spxextra{in module nn.functional}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.functional.d_mse}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{nn.functional.}}\sphinxbfcode{\sphinxupquote{d\_mse}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{y}}}{}
Compute the gradient of the mean squared error.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Input tensor.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Target tensor.

\end{itemize}

\item[{Returns}] \leavevmode
Gradient of mean squared error.

\item[{Return type}] \leavevmode
d\_mse (float)

\end{description}\end{quote}

\end{fulllineitems}

\index{d\_relu() (in module nn.functional)@\spxentry{d\_relu()}\spxextra{in module nn.functional}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.functional.d_relu}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{nn.functional.}}\sphinxbfcode{\sphinxupquote{d\_relu}}}{\emph{\DUrole{n}{x}}}{}
Compute gradient of ReLU(x)
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Input tensor

\item[{Returns}] \leavevmode
Output tensor

\item[{Return type}] \leavevmode
out (torch.tensor)

\end{description}\end{quote}

\end{fulllineitems}

\index{d\_tanh() (in module nn.functional)@\spxentry{d\_tanh()}\spxextra{in module nn.functional}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.functional.d_tanh}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{nn.functional.}}\sphinxbfcode{\sphinxupquote{d\_tanh}}}{\emph{\DUrole{n}{x}}}{}
Compute gradient of tanh(x)
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Input tensor

\item[{Returns}] \leavevmode
Output tensor

\item[{Return type}] \leavevmode
out (torch.tensor)

\end{description}\end{quote}

\end{fulllineitems}

\index{mse() (in module nn.functional)@\spxentry{mse()}\spxextra{in module nn.functional}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.functional.mse}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{nn.functional.}}\sphinxbfcode{\sphinxupquote{mse}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{y}}}{}
Compute the mean squared error.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Input tensor.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Target tensor.

\end{itemize}

\item[{Returns}] \leavevmode
Mean squared error.

\item[{Return type}] \leavevmode
ms\_error (torch.tensor)

\end{description}\end{quote}

\end{fulllineitems}

\index{relu() (in module nn.functional)@\spxentry{relu()}\spxextra{in module nn.functional}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.functional.relu}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{nn.functional.}}\sphinxbfcode{\sphinxupquote{relu}}}{\emph{\DUrole{n}{x}}}{}
Compute ReLU(x)
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Input tensor

\item[{Returns}] \leavevmode
Output tensor

\item[{Return type}] \leavevmode
out (torch.tensor)

\end{description}\end{quote}

\end{fulllineitems}

\index{tanh() (in module nn.functional)@\spxentry{tanh()}\spxextra{in module nn.functional}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.functional.tanh}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{nn.functional.}}\sphinxbfcode{\sphinxupquote{tanh}}}{\emph{\DUrole{n}{x}}}{}
Compute tanh(x).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Input tensor

\item[{Returns}] \leavevmode
Output tensor

\item[{Return type}] \leavevmode
out (torch.tensor)

\end{description}\end{quote}

\end{fulllineitems}



\subsection{nn.linear module}
\label{\detokenize{nn:module-nn.linear}}\label{\detokenize{nn:nn-linear-module}}\index{module@\spxentry{module}!nn.linear@\spxentry{nn.linear}}\index{nn.linear@\spxentry{nn.linear}!module@\spxentry{module}}\index{Layer (class in nn.linear)@\spxentry{Layer}\spxextra{class in nn.linear}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.linear.Layer}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{nn.linear.}}\sphinxbfcode{\sphinxupquote{Layer}}}
Bases: {\hyperref[\detokenize{nn:nn.module.Module}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{nn.module.Module}}}}}
\index{param() (nn.linear.Layer method)@\spxentry{param()}\spxextra{nn.linear.Layer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.linear.Layer.param}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{param}}}{}{}
Return the params of the Layer.

\end{fulllineitems}

\index{update\_param() (nn.linear.Layer method)@\spxentry{update\_param()}\spxextra{nn.linear.Layer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.linear.Layer.update_param}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_param}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
Update the params of the Layer based on the cached gradients

\end{fulllineitems}


\end{fulllineitems}

\index{Linear (class in nn.linear)@\spxentry{Linear}\spxextra{class in nn.linear}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.linear.Linear}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{nn.linear.}}\sphinxbfcode{\sphinxupquote{Linear}}}{\emph{\DUrole{n}{dim\_in}}, \emph{\DUrole{n}{dim\_out}}}{}
Bases: {\hyperref[\detokenize{nn:nn.linear.Layer}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{nn.linear.Layer}}}}}
\index{backward() (nn.linear.Linear method)@\spxentry{backward()}\spxextra{nn.linear.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.linear.Linear.backward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{backward}}}{\emph{\DUrole{n}{dy}}}{}
Compute gradients of input and parameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{dy}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Backpropagated gradient from the next layer.

\item[{Returns}] \leavevmode
Gradient.

\item[{Return type}] \leavevmode
output (torch.tensor)

\end{description}\end{quote}

\end{fulllineitems}

\index{forward() (nn.linear.Linear method)@\spxentry{forward()}\spxextra{nn.linear.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.linear.Linear.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}}{}
Calculate output.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Input tensor of size (batch\_size, input\_dim)

\item[{Returns}] \leavevmode
Output tensor of size (batch\_size, output\_dim)

\item[{Return type}] \leavevmode
output (torch.tensor)

\end{description}\end{quote}

\end{fulllineitems}

\index{param() (nn.linear.Linear method)@\spxentry{param()}\spxextra{nn.linear.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.linear.Linear.param}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{param}}}{}{}
Get parameters of the linear layer from the cache.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
weight and bias of linear layer.

\item[{Return type}] \leavevmode
w, b (torch.tensor)

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{empty() (in module nn.linear)@\spxentry{empty()}\spxextra{in module nn.linear}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.linear.empty}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{nn.linear.}}\sphinxbfcode{\sphinxupquote{empty}}}{\emph{*size}, \emph{*}, \emph{out=None}, \emph{dtype=None}, \emph{layout=torch.strided}, \emph{device=None}, \emph{requires\_grad=False}, \emph{pin\_memory=False}}{{ $\rightarrow$ Tensor}}
Returns a tensor filled with uninitialized data. The shape of the tensor is
defined by the variable argument \sphinxcode{\sphinxupquote{size}}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{size}} (\sphinxstyleliteralemphasis{\sphinxupquote{int...}}) \textendash{} a sequence of integers defining the shape of the output tensor.
Can be a variable number of arguments or a collection like a list or tuple.

\item[{Keyword Arguments}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{out}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} the output tensor.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{dtype}} (\sphinxcode{\sphinxupquote{torch.dtype}}, optional) \textendash{} the desired data type of returned tensor.
Default: if \sphinxcode{\sphinxupquote{None}}, uses a global default (see \sphinxcode{\sphinxupquote{torch.set\_default\_tensor\_type()}}).

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{layout}} (\sphinxcode{\sphinxupquote{torch.layout}}, optional) \textendash{} the desired layout of returned Tensor.
Default: \sphinxcode{\sphinxupquote{torch.strided}}.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{device}} (\sphinxcode{\sphinxupquote{torch.device}}, optional) \textendash{} the desired device of returned tensor.
Default: if \sphinxcode{\sphinxupquote{None}}, uses the current device for the default tensor type
(see \sphinxcode{\sphinxupquote{torch.set\_default\_tensor\_type()}}). \sphinxcode{\sphinxupquote{device}} will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{requires\_grad}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If autograd should record operations on the
returned tensor. Default: \sphinxcode{\sphinxupquote{False}}.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{pin\_memory}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: \sphinxcode{\sphinxupquote{False}}.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{memory\_format}} (\sphinxcode{\sphinxupquote{torch.memory\_format}}, optional) \textendash{} the desired memory format of
returned Tensor. Default: \sphinxcode{\sphinxupquote{torch.contiguous\_format}}.

\end{itemize}

\end{description}\end{quote}

Example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{torch}\PYG{o}{.}\PYG{n}{empty}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+go}{tensor(1.00000e\PYGZhy{}08 *}
\PYG{g+go}{       [[ 6.3984,  0.0000,  0.0000],}
\PYG{g+go}{        [ 0.0000,  0.0000,  0.0000]])}
\end{sphinxVerbatim}

\end{fulllineitems}



\subsection{nn.loss module}
\label{\detokenize{nn:module-nn.loss}}\label{\detokenize{nn:nn-loss-module}}\index{module@\spxentry{module}!nn.loss@\spxentry{nn.loss}}\index{nn.loss@\spxentry{nn.loss}!module@\spxentry{module}}\index{Loss (class in nn.loss)@\spxentry{Loss}\spxextra{class in nn.loss}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.loss.Loss}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{nn.loss.}}\sphinxbfcode{\sphinxupquote{Loss}}}
Bases: {\hyperref[\detokenize{nn:nn.module.Module}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{nn.module.Module}}}}}

The Loss Module is used to implement a node in the network that computes the loss.
For the computation of any function the respective functional from functional.py should be used.
\index{backward() (nn.loss.Loss method)@\spxentry{backward()}\spxextra{nn.loss.Loss method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.loss.Loss.backward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{backward}}}{}{}~\begin{description}
\item[{Backward pass.}] \leavevmode
The backward method can be implemented in the generic Loss class
as it should be the same for all Loss Modules.

\end{description}
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
Backpropagated gradient from the next layer.

\item[{Return type}] \leavevmode
dy (torch.tensor)

\end{description}\end{quote}

\end{fulllineitems}

\index{forward() (nn.loss.Loss method)@\spxentry{forward()}\spxextra{nn.loss.Loss method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.loss.Loss.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{y}}}{}
Compute the loss.
:param x: Input tensor.
:type x: torch.tensor
:param y: Target tensor.
:type y: torch.tensor

\end{fulllineitems}


\end{fulllineitems}

\index{MSELoss (class in nn.loss)@\spxentry{MSELoss}\spxextra{class in nn.loss}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.loss.MSELoss}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{nn.loss.}}\sphinxbfcode{\sphinxupquote{MSELoss}}}
Bases: {\hyperref[\detokenize{nn:nn.loss.Loss}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{nn.loss.Loss}}}}}
\index{forward() (nn.loss.MSELoss method)@\spxentry{forward()}\spxextra{nn.loss.MSELoss method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.loss.MSELoss.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{y}}}{}
Compute the mean squared error.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Input tensor.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Target tensor.

\end{itemize}

\item[{Returns}] \leavevmode
Mean squared error.

\item[{Return type}] \leavevmode
output (torch.tensor)

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsection{nn.module module}
\label{\detokenize{nn:module-nn.module}}\label{\detokenize{nn:nn-module-module}}\index{module@\spxentry{module}!nn.module@\spxentry{nn.module}}\index{nn.module@\spxentry{nn.module}!module@\spxentry{module}}\index{Module (class in nn.module)@\spxentry{Module}\spxextra{class in nn.module}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.module.Module}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{nn.module.}}\sphinxbfcode{\sphinxupquote{Module}}}
Bases: \sphinxcode{\sphinxupquote{object}}
\index{backward() (nn.module.Module method)@\spxentry{backward()}\spxextra{nn.module.Module method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.module.Module.backward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{backward}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\end{fulllineitems}

\index{forward() (nn.module.Module method)@\spxentry{forward()}\spxextra{nn.module.Module method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.module.Module.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\end{fulllineitems}


\end{fulllineitems}



\subsection{nn.sequential module}
\label{\detokenize{nn:module-nn.sequential}}\label{\detokenize{nn:nn-sequential-module}}\index{module@\spxentry{module}!nn.sequential@\spxentry{nn.sequential}}\index{nn.sequential@\spxentry{nn.sequential}!module@\spxentry{module}}\index{Sequential (class in nn.sequential)@\spxentry{Sequential}\spxextra{class in nn.sequential}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.sequential.Sequential}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{nn.sequential.}}\sphinxbfcode{\sphinxupquote{Sequential}}}{\emph{\DUrole{n}{modules}}, \emph{\DUrole{n}{loss\_fn}}}{}
Bases: {\hyperref[\detokenize{nn:nn.module.Module}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{nn.module.Module}}}}}
\index{backward() (nn.sequential.Sequential method)@\spxentry{backward()}\spxextra{nn.sequential.Sequential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.sequential.Sequential.backward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{backward}}}{}{}
Perform backward pass.

\end{fulllineitems}

\index{forward() (nn.sequential.Sequential method)@\spxentry{forward()}\spxextra{nn.sequential.Sequential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.sequential.Sequential.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}}{}
Perform forward pass.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Input tensor

\item[{Returns}] \leavevmode
Output tensor

\item[{Return type}] \leavevmode
out (torch.tensor)

\end{description}\end{quote}

\end{fulllineitems}

\index{loss() (nn.sequential.Sequential method)@\spxentry{loss()}\spxextra{nn.sequential.Sequential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.sequential.Sequential.loss}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{loss}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{y}}}{}
Compute loss between two tensors.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Input tensor

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Target tensor

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{print() (nn.sequential.Sequential method)@\spxentry{print()}\spxextra{nn.sequential.Sequential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.sequential.Sequential.print}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print}}}{}{}
Print model architecture.

\end{fulllineitems}

\index{test\_step() (nn.sequential.Sequential method)@\spxentry{test\_step()}\spxextra{nn.sequential.Sequential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.sequential.Sequential.test_step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{test\_step}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{y}}}{}
\end{fulllineitems}

\index{training\_step() (nn.sequential.Sequential method)@\spxentry{training\_step()}\spxextra{nn.sequential.Sequential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.sequential.Sequential.training_step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{training\_step}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{y}}}{}
Training step.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Input tensor

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Target tensor

\end{itemize}

\item[{Returns}] \leavevmode
computed loss

\item[{Return type}] \leavevmode
loss (torch.tensor)

\end{description}\end{quote}

\end{fulllineitems}

\index{update\_params() (nn.sequential.Sequential method)@\spxentry{update\_params()}\spxextra{nn.sequential.Sequential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.sequential.Sequential.update_params}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_params}}}{\emph{\DUrole{n}{optim}}, \emph{\DUrole{n}{lr}}}{}~\begin{description}
\item[{Update the parameters of the network iteratively}] \leavevmode
according to the cached gradients at each module.

\end{description}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{optim}} (\sphinxstyleliteralemphasis{\sphinxupquote{string}}) \textendash{} The optimizer to use. options are ‘adam’ or ‘sgd’

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{lr}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} Learning rate

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{validation\_step() (nn.sequential.Sequential method)@\spxentry{validation\_step()}\spxextra{nn.sequential.Sequential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{nn:nn.sequential.Sequential.validation_step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{validation\_step}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{y}}}{}
Validation step.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Input tensor

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.tensor}}) \textendash{} Target tensor

\end{itemize}

\item[{Returns}] \leavevmode
computed loss

\item[{Return type}] \leavevmode
loss (torch.tensor)

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsection{Module contents}
\label{\detokenize{nn:module-nn}}\label{\detokenize{nn:module-contents}}\index{module@\spxentry{module}!nn@\spxentry{nn}}\index{nn@\spxentry{nn}!module@\spxentry{module}}

\section{trainer module}
\label{\detokenize{trainer:module-trainer}}\label{\detokenize{trainer:trainer-module}}\label{\detokenize{trainer::doc}}\index{module@\spxentry{module}!trainer@\spxentry{trainer}}\index{trainer@\spxentry{trainer}!module@\spxentry{module}}\index{Trainer (class in trainer)@\spxentry{Trainer}\spxextra{class in trainer}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{trainer:trainer.Trainer}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{trainer.}}\sphinxbfcode{\sphinxupquote{Trainer}}}{\emph{\DUrole{n}{nb\_epochs}}}{}
Bases: \sphinxcode{\sphinxupquote{object}}
\index{fit() (trainer.Trainer method)@\spxentry{fit()}\spxextra{trainer.Trainer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{trainer:trainer.Trainer.fit}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fit}}}{\emph{\DUrole{n}{model}}, \emph{\DUrole{n}{x\_train}}, \emph{\DUrole{n}{y\_train}}, \emph{\DUrole{n}{x\_val}}, \emph{\DUrole{n}{y\_val}}, \emph{\DUrole{n}{batch\_size}\DUrole{o}{=}\DUrole{default_value}{32}}, \emph{\DUrole{n}{lr}\DUrole{o}{=}\DUrole{default_value}{0.01}}, \emph{\DUrole{n}{optim}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}sgd\textquotesingle{}}}, \emph{\DUrole{n}{verbose}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{print\_every}\DUrole{o}{=}\DUrole{default_value}{32}}}{}
Train the model on the specified data and print the training and validation loss and accuracy.
:param model: Module. Model to train
:param dl\_train: DataLoader. DataLoader containing the training data
:param dl\_val: DataLoader. DataLoader containting the validation data
:param verbose: bool. Whether or not to output training information

\end{fulllineitems}

\index{test() (trainer.Trainer method)@\spxentry{test()}\spxextra{trainer.Trainer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{trainer:trainer.Trainer.test}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{test}}}{\emph{\DUrole{n}{model}}, \emph{\DUrole{n}{x\_test}}, \emph{\DUrole{n}{y\_test}}, \emph{\DUrole{n}{batch\_size}\DUrole{o}{=}\DUrole{default_value}{32}}, \emph{\DUrole{n}{test\_verbose}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
Test the model on the specified data
:param model: Module. Model to train
:param dl\_test: DataLoader. DataLoader containting the test data
:param test\_verbose: bool. Whether the test result should be printed

\end{fulllineitems}


\end{fulllineitems}



\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{n}
\item\relax\sphinxstyleindexentry{nn}\sphinxstyleindexpageref{nn:\detokenize{module-nn}}
\item\relax\sphinxstyleindexentry{nn.activation}\sphinxstyleindexpageref{nn:\detokenize{module-nn.activation}}
\item\relax\sphinxstyleindexentry{nn.functional}\sphinxstyleindexpageref{nn:\detokenize{module-nn.functional}}
\item\relax\sphinxstyleindexentry{nn.linear}\sphinxstyleindexpageref{nn:\detokenize{module-nn.linear}}
\item\relax\sphinxstyleindexentry{nn.loss}\sphinxstyleindexpageref{nn:\detokenize{module-nn.loss}}
\item\relax\sphinxstyleindexentry{nn.module}\sphinxstyleindexpageref{nn:\detokenize{module-nn.module}}
\item\relax\sphinxstyleindexentry{nn.sequential}\sphinxstyleindexpageref{nn:\detokenize{module-nn.sequential}}
\indexspace
\bigletter{t}
\item\relax\sphinxstyleindexentry{trainer}\sphinxstyleindexpageref{trainer:\detokenize{module-trainer}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}